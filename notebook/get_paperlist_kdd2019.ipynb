{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scraping KDD2019"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "20\n",
      "40\n",
      "60\n",
      "80\n",
      "100\n",
      "120\n",
      "140\n",
      "160\n",
      "180\n",
      "200\n",
      "220\n",
      "240\n",
      "260\n",
      "280\n",
      "300\n",
      "320\n"
     ]
    }
   ],
   "source": [
    "papers = {}\n",
    "\n",
    "target_url = 'http://www.kdd.org/kdd2019/accepted-papers'\n",
    "r = requests.get(target_url)\n",
    "soup = BeautifulSoup(r.text, 'lxml')\n",
    "for i, item in enumerate( soup.select('a[href^=\"https://www.kdd.org/kdd2019/accepted-papers/view/\"]') ):\n",
    "    if i % 20 == 0: print( i )\n",
    "    title = item.get_text()\n",
    "    url = item.get('href')\n",
    "    r = requests.get(url)\n",
    "    soup = BeautifulSoup(r.text, 'lxml')\n",
    "    s = soup.find('section', {'class': 'g-pt-50 g-pb-100'})\n",
    "\n",
    "    author = s.p.text\n",
    "    author_list = [ n.strip() for n in author.split(';')[:-1]]\n",
    "    abst = s.find_all('p')[1].text\n",
    "    try:\n",
    "        yid = s.find('iframe').attrs['src'].split('/')[-1]\n",
    "    except:\n",
    "        yid = 'None'\n",
    "    try:\n",
    "        pdf = s.find('a', {'class': 'btn u-btn-primary g-brd-2 g-brd-white g-font-size-13 g-rounded-50 g-pl-20 g-mb-20 g-pr-15 g-py-9'}).get('href')\n",
    "    except:\n",
    "        pdf = 'None'\n",
    "    \n",
    "    paper = {}\n",
    "    paper['title'] = title\n",
    "    paper['author'] = author_list\n",
    "    paper['abst'] = abst\n",
    "    paper['yid'] = yid\n",
    "    paper['pdf'] = pdf\n",
    "    papers[i] = paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "321"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(papers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get scrapbox import file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import json\n",
    "from pprint import pprint\n",
    "\n",
    "f = open( '../scrapbox_file/kdd2019-paperlist.json', 'r' )\n",
    "j = json.load( f )\n",
    "\n",
    "for i, paper in papers.items():\n",
    "\n",
    "    if i > 319 : continue\n",
    "\n",
    "    page = {}\n",
    "    page['title'] = paper['title']\n",
    "\n",
    "    page['lines'] = []\n",
    "    page['lines'].append( paper['title'] )\n",
    "\n",
    "    page['lines'].append( '' )   \n",
    "    aut = ''\n",
    "    for a in paper['author']:\n",
    "        a = a.replace(' ', '_')\n",
    "        aut += '#%s; ' % a\n",
    "    page['lines'].append( aut )\n",
    "    \n",
    "    page['lines'].append( '' )\n",
    "    page['lines'].append( paper['abst'] )\n",
    "\n",
    "    if paper['yid'] == 'None':\n",
    "        page['lines'].append( '' )\n",
    "        page['lines'].append( 'The video does not exist' )    \n",
    "    else:\n",
    "        page['lines'].append( '' )\n",
    "        page['lines'].append( '[https://www.youtube.com/watch?v=%s]' % paper['yid'] )    \n",
    "    \n",
    "    if paper['pdf'] == 'None':\n",
    "        page['lines'].append( '' )\n",
    "        page['lines'].append( 'The PDF does not exist' )  \n",
    "    else:\n",
    "        page['lines'].append( '' )\n",
    "        page['lines'].append( 'PDF url: %s' % paper['pdf'] )\n",
    "\n",
    "    page['lines'].append( '' )\n",
    "    page['lines'].append( '一言でいうと(３行以内)' )\n",
    "    page['lines'].append( '' )\n",
    "    page['lines'].append( '新規性・差分' )\n",
    "    page['lines'].append( '' )\n",
    "    page['lines'].append( '手法' )\n",
    "    page['lines'].append( '' )\n",
    "    page['lines'].append( '結果' )\n",
    "    page['lines'].append( '' )\n",
    "    page['lines'].append( 'コメント' )\n",
    "\n",
    "    page['lines'].append( '' )\n",
    "    page['lines'].append( '[/icons/hr.icon]' )\n",
    "    page['lines'].append( 'External URLs' )\n",
    "    \n",
    "    page['lines'].append( '' )\n",
    "    page['lines'].append( '[/icons/hr.icon]' )\n",
    "    page['lines'].append( 'Tags' )\n",
    "    page['lines'].append( '#kdd2019' )\n",
    "        \n",
    "    j['pages'].append( page )\n",
    "    \n",
    "fw = open( '../scrapbox_file/kdd2019-paperlist-mod.json', 'w' )\n",
    "json.dump( j, fw, indent=2 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
